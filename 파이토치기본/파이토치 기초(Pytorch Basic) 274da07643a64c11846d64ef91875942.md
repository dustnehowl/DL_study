# 파이토치 기초(Pytorch Basic)

# 1. 파이토치 패키지의 기본 구성

## 1. torch

---

메인 네임스페이스이다. 텐서 등의 다양한 수학 함수가 포함되어져 있으며 Numpy와 유사한 구조를 가진다.

## 2. torch.autograd

---

자동 미분을 위한 함수들이 포함되어 있다. 자동 미분의 on/off를 제어하는 콘텍스트 매니저(enable_grad/no_grad)나 자체 미분 가능 함수를 정의할 때 사용하는 기반 클래스인 ‘Function’등이 포함되어 있다.

## 3. torch.nn

---

신경망을 구축하기 위해 다양한 데이터 구조나 레이어 등이 정이되어 있다. 예를 들어 RNN, LSTM과 같은 레이어, ReLU와 같은 활성화 함수, MSELoss와 같은 손실함수들이 있다.

## 4. torch.optim

---

확률정 경사 하강법을 중심으로 한 파라미터 최적화 알고리즘이 구현되어 있다.

## 5. torch.utils.data

---

SGD의 반복 연산을 실행할 때 사용하는 미니 배치용 유틸리티 함수가 포함되어 있다.

## 6. torch.onnx

---

onnx의 포맷으로 모델을 export할 때 사용한다. onnx는 서로 다른 딥 러닝 프레임워크 간에 모델을 공유할 때 사용하는 포맷이다.

# 2. 텐서 조작하기

벡터, 행렬, 텐서의 개념에 대해서 이해하고, Numpy와 파이토치로 벡터, 행렬, 텐서를 다루는 방법에 대해서 이해하고자 한다.

## 1. 벡터, 행렬 그리고 텐서

---

### 1) 벡터, 행렬, 텐서 그림으로 이해하기

![스크린샷 2023-06-20 오후 4.06.25.png](%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A9%E1%84%8E%E1%85%B5%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9(Pytorch%20Basic)%20274da07643a64c11846d64ef91875942/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-06-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.06.25.png)

딥 러닝을 하게 되면 다루게 되는 가장 기본적인 단위는 벡터, 행렬, 텐서이다. 차원이 없는 값을 스칼라, 1차원으로 구성된 값을 벡터라고 한다.

2차원으로 구성된 값을 행렬이라고 한다. 그리고 3차원이 되면 우리는 텐서라고 부른다. 우리는 3차원 세상에 살고 있으므로, 4차원 이상부터는 머리로 생각하기는 어렵다. 4차원은 3차원의 텐서를 위로 쌓아 올린 모습으로 상상해볼 수 있다.

5차원은 그 4차원을 다시 옆으로 확장한 모습으로 상상할 수 있다. 6차원도 마찬가지!

데이터 사이언스 분야 한정으로 3차원 이상의 텐서는 그냥 다차원 행렬 또는 배열로 간주할 수 있다. 또는 주로 3차원 이상을 텐서라고 하긴 하지만, 1차원 벡터나 2차원인 행렬도 텐서라고 표현하기도 한다.

### 2) Pytorch Tensor Shape Convention

딥 러닝을 할 때 다루고 있는 행렬 또는 텐서의 크기를 고려하는 것은 항상 중요하다. 여기서는 앞으로 행렬과 텐서의 크기를 표현할 때 다음과 같은 방법으로 표기한다. 앞으로 다루게 될 텐서 중 전형적인 2차원 텐서의 예이다.

**2D Tensor(Typical Simple Setting)**

![스크린샷 2023-06-20 오후 4.10.32.png](%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A9%E1%84%8E%E1%85%B5%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9(Pytorch%20Basic)%20274da07643a64c11846d64ef91875942/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-06-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.10.32.png)

위의 경우는 2차원 텐서의 크기 |t|를 batch size * dimension으로 표현했을 경우이다.

컴퓨터는 훈련 데이터를 하나씩 처리하는 것보다 보통 덩어리로 처리한다.(300개에서 64개씩 꺼내서 처리한다고 한다면 batch_size 는 64이다.) 

**3D Tensor(Typical Computer Vision) - 비전 분야에서의 3차원 텐서**

![스크린샷 2023-06-20 오후 4.20.37.png](%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A9%E1%84%8E%E1%85%B5%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9(Pytorch%20Basic)%20274da07643a64c11846d64ef91875942/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-06-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.20.37.png)

일반적으로 자연어 처리보다 비전 분야(이미지, 영상 처리)를 하게 되면 좀 더 복잡한 텐서를 다루게 된다. 이미지는 가로, 세로가 존재한다. 그리고 여러 장의 이미지 즉 batch_size로 구성하게 되면 위와 같은 3차원의 텐서가 된다.

**3D Tensor(Typical Natural Language Processing) - NLP 분야에서의 3차원 텐서**

![스크린샷 2023-06-20 오후 4.22.32.png](%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A9%E1%84%8E%E1%85%B5%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9(Pytorch%20Basic)%20274da07643a64c11846d64ef91875942/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-06-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.22.32.png)

자연어 처리는 보통 (batch_size, 문장 길이, 단어 벡터의 차원)이라는 3차원 텐서를 사용한다.

### NLP 분야의 3D 텐서 예제로 이해하기

아래와 같이 4개의 문장으로 구성된 전체 훈련 데이터가 있다.

```python
[[나는 사과를 좋아해], [나는 바나나를 좋아해], [나는 사과를 싫어해], [나는 바나나를 싫어해]]
```

컴퓨터는 아직 이 상태로는 ‘나는 사과를 좋아해’가 단어가 1개인지 3개인지 이해하지 못한다. 우선 컴퓨터의 입력으로 사용하기 위해서는 단어별로 나눠주어야 한다.

```python
[['나는', '사과를', '좋아해'], ['나는', '바나나를', '좋아해'], ['나는', '사과를', '싫어해'], ['나는', '바나나를', '싫어해']]
```

이제 훈련 데이터의 크기는 4*3의 크기를 가지는 2D 텐서이다. 컴퓨터는 텍스트보다는 숫자를 더 잘 처리할 수 잇다. 이제 각 단어를 벡터로 만든다. 아래와 같이 단어를 3차원의 벡터로 변환한다.

```python
'나는' = [0.1, 0.2, 0.9]
'사과를' = [0.3, 0.5, 0.1]
'바나나를' = [0.3, 0.5, 0.2]
'좋아해' = [0.7, 0.6, 0.5]
'싫어해' = [0.5, 0.6, 0.7]
```

위 기준을 따라서 훈련 데이터를 재구성하면 다음과 같다.

```python
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]
```

이제 훈련 데이터는 4*3*3의 크기를 가지는 3D 텐서이다. 이제 batch_size를 2로 설정하면 다음과 같다.

```python
# 첫번째 배치 #1
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]]]

# 두번째 배치 #2
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]
```

컴퓨터는 배치 단위로 연산을 수행한다. 그리고 현재 각 배치의 텐서의 크기는 (2*3*3)이다. 이는 (batch_size, 문장길이, 단어 벡터의 차원)의 크기이다.

## 넘파이로 텐서 만들기(벡터와 행렬 만들기)

---

Pytorch로 텐서를 만들어보기 전 우선 Numpy로 텐서를 만들어보자. 우선 numpy를 라이브러리를 import 한다.

```python
import numpy as np
```

Numpy로 텐서를 만드는 방법은 간단하다. [숫자, 숫자, 숫자]와 같은 형식으로 선언하고 이를 np.array()로 감싸주면 된다.

### 1) 1D with Numpy

Numpy 로 1차원 텐서인 벡터를 만들어보자.

```python
t = np.array([0., 1., 2., 3., 4., 5., 6.])

print(t)
# [0. 1. 2. 3. 4. 5. 6.]
```

1차원 텐서인 벡터의 차원과 크기를 출력.

```python
print('Rank of t: ', t.ndim)
# Rank of t: 1
print('Shape of t: ', t.shape)
# Shape of t: (7,)
```

.ndim은 몇 차원인지를 출력한다. 1차원은 벡터, 2차원은 행렬, 3차원은 3차원 텐서이다. 현재느 벡터이므로 1차원이 출력된다. .shape는 크기를 출력한다. (7,)는 (1,7)를 의미한다.

~이후 내용 생략 (비슷한 내용으로 2D, 3D)

## 3. 파이토치 텐서 선언하기(Pytorch Tensor Allocation)

---

파이토치는 Numpy와 매우 유사하다.

```python
import torch
```

### 1) 1D with Pytorch

파이토치로 1차원 텐서인 벡터를 만들어 보자

```python
t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])
print(t)
```

dim()을 사용하면 현재 텐서의 차원을 보여준다. shape나 size()를 사용하면 크기를 확인할 수 있다.

### 2) 브로드캐스팅(Broadcasting)

두 행렬 A, B가 있다고 하자. 행렬의 덧셈과 뺄셈에 대해 알고있다면, 이 덧셈과 뺄셈을 할 때에는 두 행렬 A,B의 크기가 같아야한다. 그리고 두 행렬이 곱셈을 할 때에는 A의 마지막 차원과 B의 첫 번째 차원이 일치해야한다.

이런 규칙들이 있지만 딥 러닝을 하게되면 불가피하게 크기가 다른 행렬 또는 텐서에 대해서 사칙 연산을 수행할 필요가 있다. 이를 위해 파이토치에는 자동으로 크기를 맞춰서 연산을 수행하게 만드는 브로드캐스팅이라는 기능을제공한다.

같은 크기일 때

```python
m1 = torch.FloatTensor([[3, ,3]])
m2 = torch.FloatTensor([[2, 2]])
print(m1 + m2)
# tensor([[5. 5.]])
```

여기서 m1과 m2의 크기는 둘다 (1,2)이기 때문에 문제 없이 덧셈 연산이 가능하다.

```python
# Vector + scalar
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([3]) # [3] -> [3, 3]
print(m1 + m2)
# tensor([4., 5.]])
```

m1과 m2의 크기가 다르지만 파이토치는 m2의 크기를 (1,2)로 변경하여 연산을 수행한다. 

브로드캐스팅은 편리하지만, 자동으로 실행되는 기능이므로 주의해서 사용해야한다. 나중에 원하는 결과가 나오지 않았더라도 어디서 문제가 발생했는지 찾기가 굉장히 어려울 수 있다.

### 3) 자주 사용되는 기능들

**행렬 곱셈과 곱셈의 차이(Matrix Multiplication VS Multiplication)**

matmul()

```python
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print(m1.matmul(m2))
# tensor([[5. ], [11. ]]) (1*2)
```

**평균(Mean)**

평균을 구하는 함수는 Numpy와 매우 유사하다.

```python
t = torch.FloatTensor([1, 2])
print(t.mean())
# tensor(1.5000)
```

1과 2의 평균인 1.5가 나온다. 다음은 2차원인 행렬의 .mean()결과이다.

```python
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t.mean())
# tensor(2.5000)
```

4개 원소의 평균인 2.5가 나온다. 파라미터로 dim을 줄 수 있다.

```python
print(t.mean(dim=0))
# tensor([2., 3.])
print(t.mean(dim=1))
# tensor([1.5, 3.5])
```

**덧셈(Sum)**

평균과 동작이 동일하지만 평균이 아니라 덧셈을 한다.

```python
t = torch.tensor([[1, 2], [3, 4]])
print(t.sum())
# tensor(10.)
print(t.sum(dim=0))
# tensor([4., 6.])
```

**최대(Max)와 아그맥스(ArgMax)**

최대(Max)는 원소의 최대값을 리턴하고, 아그맥스는 최대값을 가진 인덱스를 리턴한다.

```python
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t.max())
# tensor(4.)
```

## 3. 텐서 조작하기 2

### 4) view - 원소의 수를 유지하면서 텐서의 크기 변경. 매우 중요!!!

파이토치 텐서의 뷰는 넘파이에서의 리쉐이프와 같은 역할을 한다. Reshape라는 이름에서 알 수 있듯이, 텐서의 크기를 변경해주는 역할을 한다.

```python
t = np.array([[[0, 1, 2], [3, 4, 5]],
							[[6, 7, 8], [9, 10, 11]]])
ft = torch.FloatTensor(t)
print(ft.shape)
# torch.Size([2, 2, 3])
```