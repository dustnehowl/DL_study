# EfficientNet

### Introduction

모델이 정확도를 높일 때, 일반적으로 **모델의 깊이, 너비 입력 이미지**의 크기를 조절한다. 기존에는 이 세 가지를 수동으로 조절했기 때문에 최적의 성능과 효율을 얻지 못했다. EfficientNet 은 이 3가지를 효율적으로 조절할 수 있는 compound scaling을 제안한다. 깊이, 너비, 입력 이미지 크기가 일정한 관계가 있다는 것을 실험적으로 찾아내고, 이 관계를 수식으로 만든다. Compound Scaling 방법으로 NAS구조를 수정하여 SOTA를 달성한다. ResNet이나 MobileNet등 CNN 구조에도 효과가 있다고 한다.

### Model Scaling

![스크린샷 2023-06-18 오후 7.32.29.png](EfficientNet%20d627b23e96804e039291d0b9b1288ba1/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-06-18_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_7.32.29.png)

일반적으로 모델을 Scaling 하는 방법은 (b)너비, (c)깊이, (e)입력 해상도를 조절하는 것이다. 

(a) baseline 에서 입력값은 각 레이어 함수(f)를 거쳐 최종 출력값을 생성한다. 이를 수식으로 나타내면 다음과 같다.

![스크린샷 2023-06-18 오후 7.34.14.png](EfficientNet%20d627b23e96804e039291d0b9b1288ba1/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-06-18_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_7.34.14.png)

입력값 X가 각 레이어의 함수 F() 연산을 거쳐 출력값 N이 생성된다. 이를 일반화하면 다음과 같다.

![스크린샷 2023-06-18 오후 7.34.51.png](EfficientNet%20d627b23e96804e039291d0b9b1288ba1/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-06-18_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_7.34.51.png)

여기에서, 각 레이어에서 수행하는 연산(F)를 고정하고 레이어 수, 채널 수, 입력 이미지 크기에만 집중하여 search space가 감소한다.

![스크린샷 2023-06-18 오후 8.00.55.png](EfficientNet%20d627b23e96804e039291d0b9b1288ba1/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-06-18_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_8.00.55.png)

위 수식을 보면 w, d, r 상수가 생겼다. 이 상수들의 관계를 연구한 것이 EfficientNet이다.

![스크린샷 2023-06-18 오후 8.01.50.png](EfficientNet%20d627b23e96804e039291d0b9b1288ba1/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-06-18_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_8.01.50.png)

w(너비), d(깊이), r(입력 해상도)에 따른 정확도 값이다. 그림을 보면 w, d, r 이 일정 값 이상이 되면 정확도가 빠르게 saturate하다. 그리고 w, d, r이 낮을 때는 약간만 값을 조절해도 효과가 크다. w, d, r, 하나만 조절하는 것보다 d 와 r을 함께 조절하여 최고의 효율을 찾아내는 것이 Compound Scaling이다.

### Compound Scaling

![스크린샷 2023-06-18 오후 8.10.37.png](EfficientNet%20d627b23e96804e039291d0b9b1288ba1/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-06-18_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_8.10.37.png)

알파, 베타, 감마는 small grid search로 결정되는 상수이다. 파이는 주어진 연산량에 따라 사용자가 결정하는 상수이다. FLOPs는 너비와 해상도에 따라 제곱재가 상승한다. 너비를 2배 키우면 FLOPs는 4배가 증가한다. 따라서 FLOPs는 (알파 * 베타^2 * 감마^2)^2 배 만큼 증가한다. 논문에서는 알파 * 베타^2 * 감마^2 = 2 로 제한한다. 제한된 범위에서 알파, 베타, 감마를 찾는 것이다. 총 FLOPS는 2^파이 만큼 증가한다.

### 결과

너비, 깊이, 해상도 중 하나만 조절했을 때와 compound scaling을 했을 때의 성능 비교이다.

![스크린샷 2023-06-18 오후 8.20.15.png](EfficientNet%20d627b23e96804e039291d0b9b1288ba1/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-06-18_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_8.20.15.png)

![스크린샷 2023-06-18 오후 8.20.32.png](EfficientNet%20d627b23e96804e039291d0b9b1288ba1/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-06-18_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_8.20.32.png)

동일한 연산량을 가지면서 Compound Scale의 ㅓㅇ능이 제일 좋다.